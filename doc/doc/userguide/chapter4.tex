%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Testing the BPL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unit tests}

\subsection{Introduction}

Unit tests represent an important part of a library: they make it possible to (a) verify that the available features produce the expected results, and (b) detect potential regressions in a new version of the library. The second point is particularly important when significant changes have been made to the source code in a new version.

Consequently, the number of unit tests may increase from one version to the next to cover as many aspects as possible. In other words, the number of unit tests should never decrease over time.

Even more importantly, existing tests should not be modified over time, except in very specific cases, such as an API change, and only when absolutely necessary.

\subsection{Tests categories}

For the \BPL, the unit tests are generally categorized as follows:
\begin{itemize}
\item TestLauncher:  tests for the \cxx{bpl::Launcher} class 
\item TestMemoryTree: tests for the \cxx{MemoryTree} class ("directly" tested, ie. not in the context of its regular usage inside a \cxx{bpl::vector}) 
\item TestMetaprog: tests for some C++ metaprogramming tools
\item TestMisc: miscellanous tests like \cxx{global} and \cxx{once} tags
\item TestSerialize: tests about serialization of objects.
\item TestSketch: tests that compare sketches
\item TestSort:   tests for sorting vectors
\item TestSplit: tests for the \cxx{split} BPL concept, ie. something that split input data as a parallelization scheme
\item TestSynchro: tests for synchronizing data access (e.g. \cxx{mutex})
\item TestVector: tests for using \cxx{bpl::vector}
\end{itemize}
 
Some tests are run using a launcher configured for either the \UPMEM architecture or a multicore architecture, while others do not require a launcher (see, for example, the TestMetaprog).

Note that we will not detail all the unit tests here. Readers can directly browse the source code to get an overview of the content of each test.

\subsection{Unit test framework}
 
The \BPL uses the \cxx{Catch2} unit testing framework because it is popular and actively maintained on GitHub.

The \BPL uses \cxx{Catch2} in a straightforward way, so there is nothing specific to add here. Readers can consult \verb+https://github.com/catchorg/Catch2+
 for further details if needed.

Here is a minimal test file using \cxx{Catch2} in the context of the \BPL:
\begin{minted}[bgcolor=bg]{c++}
#include <common.hpp>
#include <tasks/VectorCreation.hpp>

using namespace bpl;

TEST_CASE ("VectorCreationSimple", "[Vector]" )
{
    size_t nbErrors1 = 0;
    size_t nbErrors2 = 0;
    size_t nbitems = 1<<15;
    size_t n=0;

    Launcher<ArchUpmem> launcher {1_dpu};

    for (auto const& res : launcher.run<VectorCreation>(nbitems))  {
        nbErrors1 += res.size() == nbitems ? 0 : 1;
        size_t k=0;
        for (auto x : res) { nbErrors2 += x==(k+n) ? 0 : 1;   k++; }
        n++;
    }
    REQUIRE (nbErrors1 == 0);
    REQUIRE (nbErrors2 == 0);
}
\end{minted}

A few remarks:
\begin{itemize}
\item Include the \cxx{common.hpp} file, which provides a minimal set of includes for using the \BPL.
\item Include the header file of the task to be tested; in this example, \cxx{VectorCreation}.
\item Define a \cxx{VectorCreationSimple} test case where:
\begin{itemize}
\item Declare a \cxx{bpl::Launcher}.
\item Run the \cxx{VectorCreation} task through the launcher.
\item Iterate over each result generated by the call to \cxx{run} and check the content against the expected result.
\item Use the \cxx{REQUIRE} command to verify that everything is correct.
\end{itemize}
\end{itemize}

Almost every unit test follows this pattern.

Sometimes, a test is run with different inputs. This allows a deeper verification of the task, but only in the context of unit tests.
Here, we are concerned only with whether the test passes or fails relative to the expected result; performance is not considered at this stage and will be addressed separately in the benchmark test suite.

\subsection{Source code hierarchy}

As mentioned earlier, the \BPL relies on \cxx{CMake} as its build system. For the unit tests, the file structure is as follows:
\begin{itemize}
\item \textbf{host}: directory containing tests written using the \cxx{Catch2} framework.
\item \textbf{tasks}: directory containing the tasks (one file per task).
\end{itemize}

To test a new task, \cxx{MyTask}, follow these steps:
\begin{itemize}
\item Implement the task in a \cxx{MyTask.hpp} file and place it in the \cxx{tasks} directory.
\item In \cxx{tasks/CMakeLists.txt}, add the name of the new task to the \cxx{TASKS\_LIST} variable.
\item Add a \cxx{TEST\_CASE} in a file within the \cxx{host} directory to implement the unit test.
The task header (e.g., \cxx{tasks/MyTask.hpp}) must be included at the beginning of the file.
\end{itemize}

\subsection{Building and running the unit tests}

The unit tests are built using the \cxx{bpl-unittests.host} target in the Makefile generated by \cxx{CMake}.
From the \cxx{build} directory, you can generate the binary with:
\begin{minted}[bgcolor=bg]{shell-session}
make bpl-unittests.host
\end{minted}
You can then run the unit test suite with:
\begin{minted}[bgcolor=bg]{shell-session}
./test/unit/host/bpl-unittests.host -d y
\end{minted}
The \cxx{-d y} option is optional; it displays each test name along with its execution time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark tests}

\subsection{Introduction}

The main idea of the BIOPIM project is to implement algorithms on PIM architectures (e.g., \UPMEM) and evaluate the potential speedup.
It is therefore important to be able to compare execution times against a more standard approach, for example using multithreading.

A key feature of the \BPL is the ability to write a single source code that can run on different architectures, such as multicore and \UPMEM.
This allows us to directly compare the execution times of the exact same algorithm implementation across different architectures.
For example, we could write the following:
\begin{minted}[bgcolor=bg]{c++}
{
  auto t0 = timestamp()
  Launcher<Multicore>{}.run<SomeAlgorithm> (data);
  auto t1 = timestamp()
  Launcher<Upmem>{}.run<SomeAlgorithm> (data);
  auto t2 = timestamp()
}
\end{minted}
Here, $t1 - t0$ represents the execution time of the algorithm on a multicore architecture, while $t2 - t1$ represents the execution time on a \UPMEM architecture.
This makes it possible to generate benchmark reports showing both the scalability of a given architecture and the speedup of one architecture (e.g., \UPMEM) compared to another (e.g., multicore).

In practice, using the \UPMEM part of the \BPL (and the UPMEM SDK under the hood) introduces some overhead, such as broadcasting data between the host and the DPUs—similar to sending data over a network.
Benchmarking the \BPL is therefore important to quantify the cost of these overheads. The \BPL can generate statistics about these overheads, allowing experts to assess whether a particular overhead is significant.

The tests used for benchmarking the \BPL are a subset of the unit tests, but with different input data.
This approach has the advantage that no additional tests need to be written specifically for benchmarking, since the unit tests already cover the functionality.
Keep in mind, however, that during benchmarking, we do not verify the correctness of the results—we are only interested in execution times. Correctness is ensured during the unit testing stage.

Benchmark reporting requires much more information than unit testing, where we only need to know whether a test passes or fails.
During benchmarking, we may need to generate various types of visualizations to assist the expert.
The \BPL provides a Jupyter notebook that takes as input the benchmark tests output and produces a PDF report containing multiple graphics, which we will now describe.

\subsection{The benchmark report}

The Jupyter notebook to be launched is located in \cxx{test/benchmark/notebooks} and is named \cxx{1.Benchmark.ipynb}.
It takes as input the traces generated during the execution of the benchmark.

After some introductory text, a section named after each test is generated with the following sub-sections:
\begin{itemize}
\item \textbf{Description}: a short description of the test. This description should be written as a comment in the test source code following specific metadata rules.
\item \textbf{Remarks}: additional comments or notes, also included as comments in the source code.
\item \textbf{Benchmark-input}: defines the input range, i.e., the values for which the test will be executed.
\item \textbf{Benchmark-split}: indicates whether an input argument has been "split" among process units.
\item \textbf{Source code}: the C++ source code of the test. Including the code in the report is useful, but it is recommended to avoid excessively long snippets.
\item \textbf{Multicore vs \UPMEM}: compares execution times on a logarithmic scale. One graph is generated per input value.
The x-axis represents threads in the case of multicore architectures and ranks in the case of UPMEM.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure1.png}
\end{figure}
\item \textbf{Speedup \UPMEM vs Multicore}: the ratio of execution time for 1 thread on multicore divided by the execution time for 1 rank on \UPMEM.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure2.png}
\end{figure}
\item \textbf{Scalability per architecture}: for both multicore and \UPMEM, this section displays scalability per process unit.
 For a given number $N$ of process units (threads or ranks, ie. 16*64*tasklets per rank), the ratio of the execution time for 1 process unit to that for $N$ process units is shown.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure3.png}
\end{figure}
\item \textbf{Overheads for the \UPMEM architecture}: the \BPL provides statistics for overheads specific to \UPMEM. These timings are measured from the host's perspective, not the DPU's.
\begin{itemize}
\item \textbf{launch}: execution time of the task, i.e., the execution of the \cxx{dpu\_launch} function (with \cxx{DPU\_SYNCHRONOUS} argument).
\item \textbf{pre}: preparation and broadcasting of the input data from the host to the DPU (including call to scatter/gather function \cxx{dpu\_push\_sg\_xfer})
\item \textbf{post}: retrieval of output metadata from the DPU to the host (call to \cxx{dpu\_push\_xfer} function). This metadata is used to reconstruct the task result.
\item \textbf{result}: preparation and transmission of the task’s output data back to the host (calls to \cxx{dpu\_copy\_from\_mrams} for instance)
\item \textbf{unknown}: parts that cannot be measured in the previous categories (e.g., destructors) are included here.
\end{itemize}
In the following example, preparing and sending the input data to the DPU takes a significant amount of time compared to the actual processing.
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{images/figure4.png}
\end{figure}
\end{itemize}

The execution times recorded during benchmarking are summarized in tables at the end of the report.
While this may appear verbose, it can be very useful for referencing specific values from previous benchmark runs.

\subsection{Annotation of a test}
As mentioned earlier, the source code of a benchmark test must include a few annotations that are used to generate the benchmark report.
Each annotation should be written inside a C++ comment. The annotation label begins with the @ character (e.g., \cxx{@description}), followed by a : and a free text describing the annotation.
Here is a complete example of a test source file with benchmark annotations:
 \begin{minted}[bgcolor=bg]{c++}
////////////////////////////////////////////////////////////////////////
// @description: Takes a vector as input, iterates its content and 
// compute the checksum.
// @benchmark-input: 2^n for n in 20,22,24,26,28
// @benchmark-split: yes
////////////////////////////////////////////////////////////////////////
template<class ARCH>
struct VectorChecksum : bpl::Task<ARCH>
{
    USING(ARCH);

    auto operator() (vector<uint32_t> const& v)
    {
        uint64_t checksum = 0;
        for (auto x : v)  {  checksum += x;  }
        return checksum;
    }
};
\end{minted}

\subsection{Building and running the benchmark tests}

The benchmark tests are built using the \cxx{bpl-benchmark.host} target in the Makefile generated by \cxx{CMake}.
From the \cxx{build} directory, you can generate the binary with:
\begin{minted}[bgcolor=bg]{shell-session}
make bpl-benchmark.host
\end{minted}
You can then run the benchmark test suite with:
\begin{minted}[bgcolor=bg]{shell-session}
./test/benchmark/host/bpl-benchmark.host
\end{minted}
During the tests, traces are generated. These traces serve as input to the Jupyter notebook, so they need to be redirected to a file.
It is possible to use the \cxx{nohup} command to run the tests and redirect the traces to a file whose name includes the current Git commit hash:
\begin{minted}[bgcolor=bg]{shell-session}
nohup ./test/benchmark/host/bpl-benchmark.host \  
 > ../test/benchmark/notebooks/traces/traces_`git rev-parse  HEAD`.txt &
\end{minted}
By following this recommendation, it is possible to preserve benchmark traces for a given Git version, allowing a benchmark report to be regenerated later 
for an older version of the library.

\subsection{Other remarks}
Creating a benchmark test suite is not straightforward, as there is no clear rule for deciding when to add a test.
A good rule of thumb is to be fair with respect to the library’s features: include tests where the library performs well on the \UPMEM architecture compared to multicore (e.g., small data / heavy computation), but also include cases where \UPMEM overheads are significant (e.g., large data / light computation).

For example, \cxx{SketchJaccardDistance} compares all possible pairs of sketches, resulting in a triple loop scanning the data. This is a scenario that favors \UPMEM because the broadcast cost is small compared to the total computation time.
On the other hand, sorting a vector with an $O(n \log n)$ algorithm is likely faster on multicore, because on \UPMEM the broadcasts (host → DPU for input and DPU → host for output) can take a significant amount of time relative to the sorting itself.
Therefore, it is important to select tests for benchmarking based on the computational complexity relative to the data size to be broadcast.

Another consideration is the total execution time of the benchmark suite. Since the same test is run with multiple input parameters, execution times can grow quickly. It is therefore important not to include too many tests, or the benchmark could take an excessively long time to complete.

\subsection{Future improvements}
Each new version of the BPL should commit the raw data generated during benchmark execution.
This makes it possible to compare two versions and identify any improvements in execution time.
A dedicated Jupyter notebook can be created to visualize the differences between the two versions using graphics.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The LOC metric}

So far, we have tested the BPL through unit tests and benchmarks.
There is an additional way to evaluate the BPL, from the developer's perspective, in cases where one wants to use the UPMEM architecture: does the BPL allow for faster development compared to using the UPMEM SDK?

To ensure a fair comparison, we will assume here that the developer has equivalent knowledge of (1) the UPMEM SDK and (2) the BPL API; in other words, that they possess sufficient understanding of both APIs to write code directly using either approach.
\footnote{In practice, the learning curve for the UPMEM SDK remains more complex than that of the BPL.}

Therefore, the metric that comes to mind is the number of lines of code required to write a program, either using the UPMEM API or the BPL API.
From now on, this metric will be referred to as LOC, for "Lines Of Code."

Of course, it is not practical to write every program using both approaches in order to compare the LOC, since the goal is primarily to develop using the BPL API. However, it is still interesting to perform this exercise on a few examples.

\subsection{Example}
We will start with the task of calculating the checksum of a buffer, as this is an example provided in the UPMEM documentation. We can therefore simply write the BPL version and compare the LOC in both cases.

Here is the UPMEM SDK version. First the DPU part (with all comments removed):
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <mram.h>
#include <stdbool.h>
#include <stdint.h>
#define CACHE_SIZE 256
#define BUFFER_SIZE (1 << 16)
__mram_noinit uint8_t buffer[BUFFER_SIZE];
__host uint32_t checksum;
int main() {
  __dma_aligned uint8_t local_cache[CACHE_SIZE];
  checksum = 0;
  for (unsigned int bytes_read = 0; bytes_read < BUFFER_SIZE;) {
    mram_read(&buffer[bytes_read], local_cache, CACHE_SIZE);
    for (unsigned int byte_index = 0; (byte_index < CACHE_SIZE) 
         && (bytes_read < BUFFER_SIZE); byte_index++, bytes_read++) {
      checksum += (uint32_t)local_cache[byte_index];
    }
  }
  return checksum;
}
\end{minted}
and the host part
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <dpu.h>
#include <assert.h>
#include <stdint.h>
#include <stdio.h>
#ifndef DPU_BINARY
#define DPU_BINARY "trivial_checksum_example"
#endif
#define BUFFER_SIZE (1 << 16)
void populate_mram(struct dpu_set_t set, uint32_t nr_dpus) {
  struct dpu_set_t dpu;
  uint32_t each_dpu;
  uint8_t *buffer = malloc(BUFFER_SIZE * nr_dpus);
  DPU_FOREACH(set, dpu, each_dpu) {
    for (int byte_index = 0; byte_index < BUFFER_SIZE; byte_index++) {
      buffer[each_dpu * BUFFER_SIZE + byte_index] = (uint8_t)byte_index;
    }
    buffer[each_dpu * BUFFER_SIZE] += each_dpu; 
    DPU_ASSERT(dpu_prepare_xfer(dpu, &buffer[each_dpu * BUFFER_SIZE]));
  }
  DPU_ASSERT(dpu_push_xfer(set, DPU_XFER_TO_DPU, "buffer", 0, BUFFER_SIZE, 
  	DPU_XFER_DEFAULT));
  free(buffer);
}
void print_checksums(struct dpu_set_t set, uint32_t nr_dpus) {
  struct dpu_set_t dpu;
  uint32_t each_dpu;
  uint32_t checksums[nr_dpus];
  DPU_FOREACH(set, dpu, each_dpu) {
    DPU_ASSERT(dpu_prepare_xfer(dpu, &checksums[each_dpu]));
  }
  DPU_ASSERT(dpu_push_xfer(set, DPU_XFER_FROM_DPU, "checksum", 0, 
  	sizeof(uint32_t), DPU_XFER_DEFAULT));
  DPU_FOREACH(set, dpu, each_dpu) {
    printf("[%u] checksum = 0x%08x\n", each_dpu, checksums[each_dpu]);
  }
}
int main() {
  struct dpu_set_t set, dpu;
  DPU_ASSERT(dpu_alloc(DPU_ALLOCATE_ALL, NULL, &set));
  DPU_ASSERT(dpu_load(set, DPU_BINARY, NULL));
  DPU_ASSERT(dpu_get_nr_dpus(set, &nr_dpus));
  populate_mram(set, nr_dpus);
  DPU_ASSERT(dpu_launch(set, DPU_SYNCHRONOUS));
  print_checksums(set, nr_dpus);
  DPU_ASSERT(dpu_free(set));
  return 0;
}
}\end{minted}
which totals $19+47$ lines, or $66$ in all.

Now, using the BPL API, we have:
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <bpl/core/Task.hpp>
template<class ARCH>
struct VectorChecksum : bpl::Task<ARCH>  {
    USING(ARCH);
    auto operator() (vector<uint32_t> const& v)  {
        return accumulate(v.begin(), v.end(), uint64_t{0});
    }
    static uint64_t reduce (uint64_t a, uint64_t b)  { return a+b; }
};
\end{minted}
and
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <vector>
#include <cstdio>
#include <bpl/bpl.hpp>
int main() {
    std::vector<uint32_t> v;  
    for (size_t i=1; i<=1<<16; i++)  {  v.push_back(i);  }
    Launcher<ArchUpmem> launcher {1_dpu};
    printf ("checksum: %ld\n", launcher.run<VectorChecksum>(split(v)));
}
}\end{minted}
which totals $9+9$ lines, or $18$ in all. 

Thus, we have 66 LOC for the UPMEM SDK and 18 LOC for the BPL, a ratio of 3.66. Note that the ratio would be 3.88 if we count characters instead of lines.

In addition to the significant reduction in code size with the BPL, code written using the BPL 
is also much more readable than with the UPMEM SDK. Indeed, reading the BPL code makes it immediately
clear that the task is to compute the checksum of a vector, which is far less obvious in the UPMEM SDK version, 
where all the low-level SDK calls reduce overall readability.

Of course, it would be worthwhile to extend this LOC comparison to other examples, but it seems fairly clear that the advantage would still favor the BPL.
The remaining point to determine is the overhead of the BPL compared to writing directly using the UPMEM SDK. 
If a BPL version were consistently 50\% slower than a SDK version, its usefulness could be limited.
In this regard, benchmark tests can provide a typology of algorithms that are likely to benefit from execution via the BPL on a UPMEM architecture, and conversely, those that might be penalized.

Related to readability, it is also conceivable that a developer could implement a simple algorithm using the BPL to run on a UPMEM architecture 
without needing to know the UPMEM SDK, which would represent a significant benefit of the BPL.
