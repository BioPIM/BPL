\documentclass[a4paper,11pt]{book}

\usepackage{minted}
\usepackage[dvipsnames]{xcolor}

\usepackage{graphicx}

\usepackage[a4paper]{geometry}
\newgeometry{vmargin={30mm}, hmargin={30mm,30mm}}   % set the margins

\setlength{\parskip}{4pt}

\newcommand{\bpl}{\emph{BioPim Library} }
\newcommand{\BPL}{\emph{BPL} }
\newcommand{\UPMEM}{\emph{UPMEM} }
\newcommand{\PIM}{\emph{PIM} }
\newcommand{\eg}{e.g. }
\newcommand{\cxx}[1]{\textsf{#1}}

\definecolor{bg}{rgb}{.9, .9, .9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{BioPim Library User Guide}
\author{Erwan Drézen}
\date{\today}


\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Introduction}
The purpose of this document is to present the \bpl (also known as \BPL) to the end user, enabling the design and implementation of programs that can run on different hardware architectures, such as multicore and \PIM architectures like the \UPMEM architecture.

The underlying idea is also to accelerate the development process, particularly when targeting the UPMEM architecture, 
by encapsulating all low-level SDK calls within the library. The expected advantage is to avoid discouraging developers with what might be perceived as an overly low-level SDK, thereby reaching a broader developer community.

This document also outlines the compilation toolchain requirements and provides the fundamentals for creating programs from scratch.

The \BPL is written in C++, and therefore programs that use it should also be written in this language

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{First steps}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A first example}

\subsection{Task and launcher}

As an introduction, here is a small program using the \BPL that illustrates the overall philosophy of the library.
This program simply computes the sum of the elements of an input integer vector.
To do so, one must implement a struct or class that provides a method performing the actual computation; this method is an overload of the 
\cxx{operator()} function.
In the following, we will often refer to such a structure as a \textbf{task}. In this example, the task is a structure named \cxx{Sum}.

\begin{minted}[bgcolor=bg]{c++}
template<typename ARCH>
struct Sum
{
    USING(ARCH);

    int32_t operator() (vector<int32_t> const& in) const 
    {
        int32_t result = 0 ;
        for (auto n : in)  { result += n; }
        return result ;
    }
} ;
\end{minted}

For the moment, we do not discuss the \cxx{USING} macro, which will be explained later.

A few important preliminary remarks:
\begin{itemize}
	
	\item The \cxx{Sum} struct must be templated with a parameter \cxx{ARCH} that represents the hardware architecture on which the task will be executed. As a consequence, the implementation of the task should be agnostic with respect to the actual architecture that will ultimately be used.  
	Following this approach, the task can, in theory, be executed on potentially very different kinds of hardware architectures.

	\item The code itself is rather straightforward: it receives an \cxx{in} vector as an argument, which is iterated over while updating the overall sum of its elements.  
	The important point here is that the only data\footnote{There may be some exceptions to this remark.} that can be processed by the task are the input arguments of the \cxx{operator()} method; for instance, there is no way to retrieve data from a file in a filesystem.  
	On the other hand, the body of the task may define new objects (allocated on the execution stack) that can be returned if needed, such as the \cxx{result} variable holding the output of the task.

	\item Since we aim to design fast algorithms, we may want to leverage parallelization features such as threads on a multicore architecture.  
	Note that, from the task’s point of view (here, our \cxx{Sum} struct), there is no reference to threads or other parallelization mechanisms; the parallelization scheme is handled at another level.  
	In other words, the task is agnostic with respect to its own parallelization: it simply receives input data, processes it, and returns a result.

	\item The \BPL requires that the task (the \cxx{Sum} struct in our example) be placed in a header file following a specific naming convention.  
	That is, the header file must be named \cxx{X.hpp} if the structure is named \cxx{X}. In our example, the header file must therefore be named \cxx{Sum.hpp}.
\end{itemize}

Once we have written our \cxx{Sum} task, the \BPL provides mechanisms to use it, that is, to invoke it and retrieve a result.
To do so, the \BPL introduces the concept of a \textbf{launcher}, an object that takes as parameters a task (via the name of the structure that implements its logic) and the input arguments to be processed.

This is achieved through the \cxx{bpl::Launcher} class.
This class is responsible for handling the hardware architecture to be used; therefore, when creating a launcher, it must be instantiated with a template argument representing the target hardware architecture.
For instance, if we want to use a multicore architecture, we would write
\begin{minted}[bgcolor=bg]{c++}
Launcher<ArchMulticore> launcher;
\end{minted}

If we want to use an \UPMEM architecture, we would write
\begin{minted}[bgcolor=bg]{c++}
Launcher<ArchUpmem> launcher;
\end{minted}

At this point, we are done with specifying the hardware architecture to be used: it is defined through the instantiation of the \cxx{Launcher}, and only there.

To run our \cxx{Sum} task, we use the \cxx{run} method of the \cxx{Launcher} class.
\begin{minted}[bgcolor=bg]{c++}
vector<int32_t> myvector = {1,2,3,5,8};
auto results = launcher.run<Sum> (myvector);
\end{minted}

Note the following:
\begin{itemize}
  \item The task to be executed is provided as a template argument to the \cxx{Launcher::run} method.
  \item The arguments (\cxx{myvector} in the example) are passed as parameters to the instantiated version of \cxx{Launcher::run}.
  \item The result of the execution of \cxx{Sum::operator()} is retrieved in the \cxx{results} variable.  
  At this point, note the \emph{s} in \cxx{results}: it turns out that multiple results can be retrieved, rather than a single one
  (i.e., a simple \cxx{int32\_t} in our \cxx{Sum} example).
\end{itemize}


To run this example, we can put everything into a \cxx{main.cpp} file containing the following code:
\begin{minted}[bgcolor=bg]{c++}
#include <bpl/bpl.hpp>
#include <Sum.hpp>
#include <vector>

int main()
{
    std::vector<int32_t> myvector = {1,2,3,5,8};

    bpl::Launcher<bpl::ArchMulticore> launcher;
    auto results = launcher.run<Sum> (myvector);
    
    return 0 ;
}
\end{minted}
Note the inclusion of the \cxx{bpl.hpp} file to use the API of the \cxx{Launcher} class.
We also need to include the \cxx{Sum.hpp} file, where our task is defined; indeed, this definition is required in order to call the \cxx{Launcher::run} method with \cxx{Sum} as a template argument.

A one-liner version would be
\begin{minted}[bgcolor=bg]{c++}
#include <bpl/bpl.hpp>
#include <Sum.hpp>
#include <vector>

int main()
{
    auto results = bpl::Launcher<bpl::ArchMulticore>{}.run<Sum> (
        std::vector<int32_t> {1,2,3,5,8}
    );
    return 0 ;
}
\end{minted}

Note that we could have used a different architecture when declaring our launcher, for instance by using \cxx{ArchUpmem} instead:
\begin{minted}[bgcolor=bg]{c++}
#include <bpl/bpl.hpp>
#include <Sum.hpp>
#include <vector>

int main()
{
    auto results = bpl::Launcher<bpl::ArchUpmem>{}.run<Sum> (
        std::vector<int32_t> {1,2,3,5,8});
    return 0 ;
}
\end{minted}

Now, our program can be compiled using the CMake tool. In fact, the \BPL provides a compilation toolchain that hides many low-level details; this toolchain is an important part of the \BPL and will be described in detail later.

From the end-user’s perspective, everything we have seen so far resembles regular C++ development.
We implement a small algorithm (summing the items of a vector) as a functor—that is, a struct with an overloaded \cxx{operator()} method.
We then use the \cxx{bpl::Launcher} class to run it. Nothing in our code is specific to low-level details of the \UPMEM architecture, for example; everything is handled by the \BPL itself.
On the other hand, we have not yet discussed how to parallelize our algorithm to take full advantage of the underlying hardware architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parallelization}

Modern hardware architectures can greatly speed up programs by parallelizing algorithms; for example, by splitting an algorithm into small pieces that can be processed simultaneously and then aggregating the final result from the results of each piece.

In multicore architectures, this is often achieved using the well-known concept of a thread. Broadly speaking, using 
N threads can potentially divide the execution time of an algorithm by 
N, since each thread can run on a specific CPU of the underlying architecture.
On the other hand, the \UPMEM architecture uses the concept of a tasklet, which is similar at first glance to a thread but with significant differences that we will examine later.
For now, we introduce the concept of a \textbf{process unit} to unify both concepts of thread and tasklet—that is, an entity that can execute a portion of a program on part of the underlying hardware architecture.

One of the main challenges in designing the \BPL was to allow developers to write a single codebase that can run on different architectures (currently mainly multicore CPUs and \UPMEM).
This means that, when implementing a task, the underlying architecture is known only through the template parameter \cxx{ARCH}, so no assumptions should be made about a specific architecture.
It is only when a \cxx{bpl::Launcher} is declared with a specific architecture and the \cxx{Launcher::run} method is called for the target class that all the specifics of the underlying architecture are taken into account.

Consequently, the \BPL design had to accommodate different parallelization schemes, both for multicore and \UPMEM environments.
This is the role of the \cxx{bpl::Launcher} class: a launcher not only knows about the underlying architecture but also knows how to parallelize the task for the target architecture.

Another consideration is how many resources we want to allocate to run our task. On a 32-core architecture, for example, we might want to use only 8 cores.
This can be achieved by providing the number of process units (threads, in this case) to use as an argument to the constructor of \cxx{bpl::Launcher}.

\begin{minted}[bgcolor=bg]{c++}
Launcher<ArchMulticore> launcher (Thread{8});
\end{minted}
which can also be writen with
\begin{minted}[bgcolor=bg]{c++}
// launcher will use 8 process units (i.e. threads)
Launcher<ArchMulticore> launcher (8_thread);  
\end{minted}
So 8 threads will be used when calling \verb+launcher.run<Sum>(myvector)+, each performing the same summation on the input vector.

When using the \UPMEM architecture, it is possible to specify the amount of resources by setting the number of ranks or DPUs, e.g.,
\begin{minted}[bgcolor=bg]{c++}
// launcher1 will use 20*64*16 process units (i.e. tasklets)
Launcher<ArchUpmem> launcher1 (20_rank);  
// launcher2 will use 500*16 process units (i.e. tasklets)
Launcher<ArchUpmem> launcher2 (500_dpu);   
\end{minted}

What we have just done is define how many resources a launcher can use and the corresponding number of process units running the task.
However, each process unit will process the same input data and therefore produce the same result.
We now need to specify how to parallelize the algorithm. The \BPL provides a solution by splitting the input arguments of the task and assigning each specific split to a dedicated process unit.
In our example, the input vector \cxx{in} will be divided into sub-vectors, and each sub-vector (potentially empty) will be provided as an argument to a separate instantiation of the \cxx{Sum} structure—one per process unit.
This parallelization scheme can be easily applied by wrapping the \cxx{myvector} argument with the \cxx{bpl::split} method.
\begin{minted}[bgcolor=bg]{c++}
auto results = launcher.run<Sum> (split(myvector));
\end{minted}

Now, a word about the 's' in \cxx{results}: each process unit runs an instance of \cxx{Sum} and produces a result.
Therefore, if we have N process units, \cxx{results} will be a vector of 
N \verb+int32_t+ objects\footnote{Note that \cxx{results} could be any iterable type rather than a full-fledged vector.}.
Thus, we could write
\begin{minted}[bgcolor=bg]{c++}
// Iterate over the result of each process unit
for (auto result : launcher.run<Sum> (split(myvector)))  
{ 
    /* do something with result */ 
}
\end{minted}

We will see later that \cxx{bpl::split} can be (template) parameterized to achieve more specific behavior. For now, we just keep in mind the default parallelization model of the \BPL: the task itself does not know about parallelization; it is up to the end user (through the launcher) to provide a specific portion of the data that will feed each instance of the task running on a given process unit.

Note that this model is quite simple and may not be sufficient for every parallelization scheme. Other models could be proposed in the future.

We have now briefly covered some of the main concepts of the \BPL, such as tasks and launchers. Next, we will go a step further by retrieving the \BPL, creating a new \BPL project, and compiling it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Getting and using the \BPL}

\subsection{Requirements}
Since one of the main target architectures of the \BPL is the \UPMEM architecture, you should install the \UPMEM SDK. In particular, the \BPL works with version 2023.2 (see \verb+https://sdk.upmem.com+
), so it is recommended to use this version.

Once installed, you need to define the \verb+UPMEM_HOME+ environment variable to point to the \UPMEM SDK, for example:
\begin{verbatim}
    UPMEM_HOME=/opt/upmem-2023.2.0
\end{verbatim}
 
You should also have a recent \verb+C/C+++ compiler installed on your system that supports at least C++20.

\subsection{Getting the \BPL}

Currently, the simplest way to obtain the \BPL is to clone the following Git repository:
\begin{verbatim}
https://github.com/BioPIM/BPL.git
\end{verbatim}

Once the repository has been cloned, you can do the following to build the unit tests from the cloned directory:
\begin{itemize}
  \item mkdir build
  \item cd build
  \item cmake ..
  \item make
\end{itemize}
During the build, you should see a message instructing you to set the \verb+DPU_BINARIES_DIR+ environment variable to a specific value.
This is mandatory in order to run the unit tests, because the \BPL needs to know where the binaries generated for the \UPMEM architecture are located (more on this later).

The unit tests\footnote{The unit tests are based on the \cxx{Catch2} framework} can be launched from the build directory as follows:
\begin{verbatim}
./test/unit/host/bpl-unittests.host
\end{verbatim}

\subsection{Creating a project from scratch}

It is possible to write a new program from this content, but this repository is primarily focused on the development of the library itself.
A useful way to create an independent \BPL project is to generate a \verb+tar.gz+ archive containing the minimal source code and \cxx{CMake} files required to build a \BPL project that can run on both multicore and \UPMEM architectures.

To do this, you can use \cxx{CMake} while specifying the name of your project. For example, if you want to name your project \cxx{MyProject}, you would do the following:
\begin{verbatim}
cmake -DNEWPROJECT_NAME=MyProject ..
\end{verbatim}
This will add a specific target, \verb+create_bpl_project_MyProject+, and the project can then be generated with:
\begin{verbatim}
make create_bpl_project_MyProject
\end{verbatim}
This will produce a \verb+MyProject.tar.gz+ archive containing the minimal resources for your project. You can then do:
\begin{enumerate}
  \item \verb+tar xvfz MyProject.tar.gz+
  \item \verb+cd MyProject+
  \item \verb+mkdir build+
  \item \verb+cd build+
  \item \verb+cmake ..+
  \item \verb+make+
\end{enumerate}
Don’t forget to set the \verb+DPU_BINARIES_DIR+ environment variable as indicated in the build output. You can then run the program with:
\begin{verbatim}
./src/main/MyProject
\end{verbatim}

\subsection{Compilation toolchain with the \BPL}
The \BPL aims to make the code agnostic with respect to the underlying architecture. This can be quite challenging for architectures like \UPMEM. Normally, a developer would have to write two programs: one for the host using standard C/C++, and one for the DPU chipset using the \UPMEM SDK.
With the \BPL, the developer does not need to worry about these details and can focus on writing a single codebase.
To achieve this, the \BPL provides a compilation toolchain based on \emph{CMake}, which handles the generation of the two binaries (one for the host and one for the DPU) whenever the \UPMEM architecture is targeted.

This compilation toolchain requires that the developer follow certain conventions:
\begin{enumerate}
\item The task is implemented as a functor, i.e., a structure that defines an \cxx{operator()} method performing the computation.
\item This structure is templated, with a \cxx{ARCH} template parameter representing the target hardware architecture.
\item The structure must be implemented in a \cxx{.hpp} file named after the structure; that is, if the structure is named \cxx{X}, the header file must be named \cxx{X.hpp} and placed in the \cxx{src/tasks} directory.
\item The task’s \cxx{operator()} method can take input parameters; these parameters are the only data the task can process.
\item The task is invoked through the \cxx{bpl::Launcher} class and its \cxx{run} method.
\item The launcher can be defined, for instance, in the program’s \cxx{main} function or in any source file located in the \cxx{src/main} directory.
\item The \cxx{bpl::Launcher} class takes the target hardware architecture as a template parameter (e.g., multicore, \UPMEM, etc.); additional arguments can be provided to the constructor to define the resources to use (number of threads, tasklets, etc.).
\item Normally, the only place where a hardware architecture is explicitly referenced is when instantiating the \cxx{bpl::Launcher} class; all other code should not rely on architecture-specific details.
\item When $N$ process units are used, the \cxx{bpl::split} function can be applied to one or more input arguments, splitting each argument so that each process unit receives a specific portion as input.
\end{enumerate}

In brief, the toolchain assumes that task-related code is placed in the \cxx{src/tasks} directory,
and launcher-related code for running one or more tasks is placed in the \cxx{src/main} directory.
Based on this structure, the \BPL can generate the required binaries: one for the multicore architecture, and one plus $N$ (where $N$ is the number of tasks) when targeting the \UPMEM architecture.

\subsection{Implementing a task with the \BPL}
As noted earlier, a task is a templated structure that looks like
\begin{minted}[bgcolor=bg]{c++}
template<typename ARCH>
struct Sum
{
    USING(ARCH);
    int32_t operator() (vector<int32_t> const& in) const 
    {
        int32_t result = 0 ;
        for (auto n : in)  { result += n; }
        return result ;
    }
} ;
\end{minted}
At first glance, it looks like standard C++, but a closer look raises a question: what is the actual class for \cxx{vector}?
We are not referring here to the \cxx{std::vector} class (assuming we have not used \cxx{using namespace std} earlier), so the compiler would report an error indicating that \cxx{vector} is unknown.

A type such as \cxx{std::vector} normally relies on dynamic allocation (i.e., \cxx{new/delete}).
This is not possible on the \UPMEM architecture, so if we want a class that behaves like \cxx{std::vector}, we must provide a specific implementation from another namespace rather than \cxx{std}.
In fact, the actual \cxx{vector} class should depend on the template parameter \cxx{ARCH}, so one could write something like
\begin{minted}[bgcolor=bg]{c++}
template<typename ARCH>
struct Sum
{
    int32_t operator() (ARCH::vector<int32_t> const& in) const 
    { /* same as before */ }
} ;
\end{minted}
This will not compile, and the correct type for \cxx{in} should instead be:
\begin{minted}[bgcolor=bg]{c++}
typename ARCH::vector<int32_t> const& 
\end{minted}
This is somewhat verbose. To avoid this, one can use the \cxx{USING} macro, which acts like a \cxx{using} directive for a set of predefined types.
With the \cxx{USING} macro, it becomes possible to skip prefixing each type (such as \cxx{vector}) with \cxx{ARCH}
\footnote{Technically, we are not using namespaces here as in \cxx{using namespace}, but rather defining type aliases.},
and we can write something like
 \begin{minted}[bgcolor=bg]{c++}
template<typename ARCH>
struct Sum
{
    USING(ARCH);
    int32_t operator() (vector<int32_t> const& in) const 
    { /* same as before */ }
};
\end{minted}
In this example, \cxx{std::vector} will be used when targeting a multicore architecture, while \cxx{bpl::vector} will be used for the \UPMEM architecture.
Here, \cxx{bpl::vector} is a specific implementation that handles the lack of dynamic allocation on \UPMEM.
Note, however, that \cxx{bpl::vector} implements only a subset of the methods available in \cxx{std::vector}.
Therefore, one should rely only on this subset to ensure that the task code remains compatible with both multicore and \UPMEM architectures.

This remark highlights an important caveat for the \BPL: the goal is to have a single codebase that works across different architectures, but this comes at a cost.
While it is convenient to use data structures like \cxx{vector} in a uniform way, it is not always possible to maintain identical semantics between multicore and \UPMEM implementations.
For example, methods in \cxx{std::vector} that rely on exceptions cannot be used on \UPMEM, as exceptions are not supported there.
As a result, functional differences may occur when running such methods on multicore versus \UPMEM architectures.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Application Programming Interface}

\section{Preliminary remarks}


\section{Structure of a \BPL project}
\subsection{CMake}
\subsection{Files hierarchy}

\section{Memory constraints}
\subsection{Stack size}
\subsection{Macro ARCH\_ALIGN}

\section{UPMEM miscellanous}
\subsection{Binary size}
% a word about NOINLINE

\section{Parallelization}
\subsection{split}

\section{ARCH macro}


\section{Customizing BPL}

\section{Task}
\subsection{Reduce}

\section{mutex}

\section{Launcher}
\subsection{Getting statistics}

\section{LauncherPool}

\section{vector and vector\_view}
\subsection{MemoryTree}

\section{Tags}
\subsection{global}
\subsection{once}

\section{Activating debug traces}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The design of the BPL}

\section{C++ metaprogrammation}

\section{Broadcast}

\section{Serialization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Testing the BPL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unit tests}

\subsection{Introduction}

Unit tests represent an important part of a library: they make it possible to (a) verify that the available features produce the expected results, and (b) detect potential regressions in a new version of the library. The second point is particularly important when significant changes have been made to the source code in a new version.

Consequently, the number of unit tests may increase from one version to the next to cover as many aspects as possible. In other words, the number of unit tests should never decrease over time.

Even more importantly, existing tests should not be modified over time, except in very specific cases, such as an API change, and only when absolutely necessary.

\subsection{Tests categories}

For the \BPL, the unit tests are generally categorized as follows:
\begin{itemize}
\item TestLauncher:  tests for the \cxx{bpl::Launcher} class 
\item TestMemoryTree: tests for the \cxx{MemoryTree} class ("directly" tested, ie. not in the context of its regular usage inside a \cxx{bpl::vector}) 
\item TestMetaprog: tests for some C++ metaprogramming tools
\item TestMisc: miscellanous tests like \cxx{global} and \cxx{once} tags
\item TestSerialize: tests about serialization of objects.
\item TestSketch: tests that compare sketches
\item TestSort:   tests for sorting vectors
\item TestSplit: tests for the \cxx{split} BPL concept, ie. something that split input data as a parallelization scheme
\item TestSynchro: tests for synchronizing data access (e.g. \cxx{mutex})
\item TestVector: tests for using \cxx{bpl::vector}
\end{itemize}
 
Some tests are run using a launcher configured for either the \UPMEM architecture or a multicore architecture, while others do not require a launcher (see, for example, the TestMetaprog).

Note that we will not detail all the unit tests here. Readers can directly browse the source code to get an overview of the content of each test.

\subsection{Unit test framework}
 
The \BPL uses the \cxx{Catch2} unit testing framework because it is popular and actively maintained on GitHub.

The \BPL uses \cxx{Catch2} in a straightforward way, so there is nothing specific to add here. Readers can consult \verb+https://github.com/catchorg/Catch2+
 for further details if needed.

Here is a minimal test file using \cxx{Catch2} in the context of the \BPL:
\begin{minted}[bgcolor=bg]{c++}
#include <common.hpp>
#include <tasks/VectorCreation.hpp>

using namespace bpl;

TEST_CASE ("VectorCreationSimple", "[Vector]" )
{
    size_t nbErrors1 = 0;
    size_t nbErrors2 = 0;
    size_t nbitems = 1<<15;
    size_t n=0;

    Launcher<ArchUpmem> launcher {1_dpu};

    for (auto const& res : launcher.run<VectorCreation>(nbitems))  {
        nbErrors1 += res.size() == nbitems ? 0 : 1;
        size_t k=0;
        for (auto x : res) { nbErrors2 += x==(k+n) ? 0 : 1;   k++; }
        n++;
    }
    REQUIRE (nbErrors1 == 0);
    REQUIRE (nbErrors2 == 0);
}
\end{minted}

A few remarks:
\begin{itemize}
\item Include the \cxx{common.hpp} file, which provides a minimal set of includes for using the \BPL.
\item Include the header file of the task to be tested; in this example, \cxx{VectorCreation}.
\item Define a \cxx{VectorCreationSimple} test case where:
\begin{itemize}
\item Declare a \cxx{bpl::Launcher}.
\item Run the \cxx{VectorCreation} task through the launcher.
\item Iterate over each result generated by the call to \cxx{run} and check the content against the expected result.
\item Use the \cxx{REQUIRE} command to verify that everything is correct.
\end{itemize}
\end{itemize}

Almost every unit test follows this pattern.

Sometimes, a test is run with different inputs. This allows a deeper verification of the task, but only in the context of unit tests.
Here, we are concerned only with whether the test passes or fails relative to the expected result; performance is not considered at this stage and will be addressed separately in the benchmark test suite.

\subsection{Source code hierarchy}

As mentioned earlier, the \BPL relies on \cxx{CMake} as its build system. For the unit tests, the file structure is as follows:
\begin{itemize}
\item \textbf{host}: directory containing tests written using the \cxx{Catch2} framework.
\item \textbf{tasks}: directory containing the tasks (one file per task).
\end{itemize}

To test a new task, \cxx{MyTask}, follow these steps:
\begin{itemize}
\item Implement the task in a \cxx{MyTask.hpp} file and place it in the \cxx{tasks} directory.
\item In \cxx{tasks/CMakeLists.txt}, add the name of the new task to the \cxx{TASKS\_LIST} variable.
\item Add a \cxx{TEST\_CASE} in a file within the \cxx{host} directory to implement the unit test.
The task header (e.g., \cxx{tasks/MyTask.hpp}) must be included at the beginning of the file.
\end{itemize}

\subsection{Building and running the unit tests}

The unit tests are built using the \cxx{bpl-unittests.host} target in the Makefile generated by \cxx{CMake}.
From the \cxx{build} directory, you can generate the binary with:
\begin{minted}[bgcolor=bg]{c++}
make bpl-unittests.host
\end{minted}
You can then run the unit test suite with:
\begin{minted}[bgcolor=bg]{c++}
./test/unit/host/bpl-unittests.host -d y
\end{minted}
The \cxx{-d y} option is optional; it displays each test name along with its execution time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark tests}

\subsection{Introduction}

The main idea of the BIOPIM project is to implement algorithms on PIM architectures (e.g., \UPMEM) and evaluate the potential speedup.
It is therefore important to be able to compare execution times against a more standard approach, for example using multithreading.

A key feature of the \BPL is the ability to write a single source code that can run on different architectures, such as multicore and \UPMEM.
This allows us to directly compare the execution times of the exact same algorithm implementation across different architectures.
For example, we could write the following:
\begin{minted}[bgcolor=bg]{c++}
{
  auto t0 = timestamp()
  Launcher<Multicore>{}.run<SomeAlgorithm> (data);
  auto t1 = timestamp()
  Launcher<Upmem>{}.run<SomeAlgorithm> (data);
  auto t2 = timestamp()
}
\end{minted}
Here, $t1 - t0$ represents the execution time of the algorithm on a multicore architecture, while $t2 - t1$ represents the execution time on a \UPMEM architecture.
This makes it possible to generate benchmark reports showing both the scalability of a given architecture and the speedup of one architecture (e.g., \UPMEM) compared to another (e.g., multicore).

In practice, using the \UPMEM part of the \BPL (and the UPMEM SDK under the hood) introduces some overhead, such as broadcasting data between the host and the DPUs—similar to sending data over a network.
Benchmarking the \BPL is therefore important to quantify the cost of these overheads. The \BPL can generate statistics about these overheads, allowing experts to assess whether a particular overhead is significant.

The tests used for benchmarking the \BPL are a subset of the unit tests, but with different input data.
This approach has the advantage that no additional tests need to be written specifically for benchmarking, since the unit tests already cover the functionality.
Keep in mind, however, that during benchmarking, we do not verify the correctness of the results—we are only interested in execution times. Correctness is ensured during the unit testing stage.

Benchmark reporting requires much more information than unit testing, where we only need to know whether a test passes or fails.
During benchmarking, we may need to generate various types of visualizations to assist the expert.
The \BPL provides a Jupyter notebook that takes as input the benchmark tests output and produces a PDF report containing multiple graphics, which we will now describe.

\subsection{The benchmark report}

The Jupyter notebook to be launched is located in \cxx{test/benchmark/notebooks} and is named \cxx{1.Benchmark.ipynb}.
It takes as input the traces generated during the execution of the benchmark.

After some introductory text, a section named after each test is generated with the following sub-sections:
\begin{itemize}
\item \textbf{Description}: a short description of the test. This description should be written as a comment in the test source code following specific metadata rules.
\item \textbf{Remarks}: additional comments or notes, also included as comments in the source code.
\item \textbf{Benchmark-input}: defines the input range, i.e., the values for which the test will be executed.
\item \textbf{Benchmark-split}: indicates whether an input argument has been "split" among process units.
\item \textbf{Source code}: the C++ source code of the test. Including the code in the report is useful, but it is recommended to avoid excessively long snippets.
\item \textbf{Multicore vs \UPMEM}: compares execution times on a logarithmic scale. One graph is generated per input value.
The x-axis represents threads in the case of multicore architectures and ranks in the case of UPMEM.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure1.png}
\end{figure}
\item \textbf{Speedup \UPMEM vs Multicore}: the ratio of execution time for 1 thread on multicore divided by the execution time for 1 rank on \UPMEM.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure2.png}
\end{figure}
\item \textbf{Scalability per architecture}: for both multicore and \UPMEM, this section displays scalability per process unit.
 For a given number $N$ of process units (threads or ranks, ie. 16*64*tasklets per rank), the ratio of the execution time for 1 process unit to that for $N$ process units is shown.
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{images/figure3.png}
\end{figure}
\item \textbf{Overheads for the \UPMEM architecture}: the \BPL provides statistics for overheads specific to \UPMEM. These timings are measured from the host's perspective, not the DPU's.
\begin{itemize}
\item \textbf{launch}: execution time of the task, i.e., the execution of the \cxx{dpu\_launch} function (with \cxx{DPU\_SYNCHRONOUS} argument).
\item \textbf{pre}: preparation and broadcasting of the input data from the host to the DPU (including call to scatter/gather function \cxx{dpu\_push\_sg\_xfer})
\item \textbf{post}: retrieval of output metadata from the DPU to the host (call to \cxx{dpu\_push\_xfer} function). This metadata is used to reconstruct the task result.
\item \textbf{result}: preparation and transmission of the task’s output data back to the host (calls to \cxx{dpu\_copy\_from\_mrams} for instance)
\item \textbf{unknown}: parts that cannot be measured in the previous categories (e.g., destructors) are included here.
\end{itemize}
In the following example, preparing and sending the input data to the DPU takes a significant amount of time compared to the actual processing.
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{images/figure4.png}
\end{figure}
\end{itemize}

The execution times recorded during benchmarking are summarized in tables at the end of the report.
While this may appear verbose, it can be very useful for referencing specific values from previous benchmark runs.

\subsection{Annotation of a test}
As mentioned earlier, the source code of a benchmark test must include a few annotations that are used to generate the benchmark report.
Each annotation should be written inside a C++ comment. The annotation label begins with the @ character (e.g., \cxx{@description}), followed by a : and a free text describing the annotation.
Here is a complete example of a test source file with benchmark annotations:
 \begin{minted}[bgcolor=bg]{c++}
//////////////////////////////////////////////////////////////////////////
// @description: Takes a vector as input, iterates its content and compute
// the checksum. The final result is reduced checksum.
// @benchmark-input: 2^n for n in 20,22,24,26,28
// @benchmark-split: yes
//////////////////////////////////////////////////////////////////////////
template<class ARCH>
struct VectorChecksum : bpl::Task<ARCH>
{
    USING(ARCH);

    auto operator() (vector<uint32_t> const& v)
    {
        uint64_t checksum = 0;
        for (auto x : v)  {  checksum += x;  }
        return checksum;
    }
};
\end{minted}
  
\subsection{Other remarks}
Creating a benchmark test suite is not straightforward, as there is no clear rule for deciding when to add a test.
A good rule of thumb is to be fair with respect to the library’s features: include tests where the library performs well on the \UPMEM architecture compared to multicore (e.g., small data / heavy computation), but also include cases where \UPMEM overheads are significant (e.g., large data / light computation).

For example, \cxx{SketchJaccardDistance} compares all possible pairs of sketches, resulting in a triple loop scanning the data. This is a scenario that favors \UPMEM because the broadcast cost is small compared to the total computation time.
On the other hand, sorting a vector with an $O(n \log n)$ algorithm is likely faster on multicore, because on \UPMEM the broadcasts (host → DPU for input and DPU → host for output) can take a significant amount of time relative to the sorting itself.
Therefore, it is important to select tests for benchmarking based on the computational complexity relative to the data size to be broadcast.

Another consideration is the total execution time of the benchmark suite. Since the same test is run with multiple input parameters, execution times can grow quickly. It is therefore important not to include too many tests, or the benchmark could take an excessively long time to complete.

\subsection{Future improvements}
Each new version of the BPL should commit the raw data generated during benchmark execution.
This makes it possible to compare two versions and identify any improvements in execution time.
A dedicated Jupyter notebook can be created to visualize the differences between the two versions using graphics.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The LOC metric}

So far, we have tested the BPL through unit tests and benchmarks.
There is an additional way to evaluate the BPL, from the developer's perspective, in cases where one wants to use the UPMEM architecture: does the BPL allow for faster development compared to using the UPMEM SDK?

To ensure a fair comparison, we will assume here that the developer has equivalent knowledge of (1) the UPMEM SDK and (2) the BPL API; in other words, that they possess sufficient understanding of both APIs to write code directly using either approach.
\footnote{In practice, the learning curve for the UPMEM SDK remains more complex than that of the BPL.}

Therefore, the metric that comes to mind is the number of lines of code required to write a program, either using the UPMEM API or the BPL API.
From now on, this metric will be referred to as LOC, for "Lines Of Code."

Of course, it is not practical to write every program using both approaches in order to compare the LOC, since the goal is primarily to develop using the BPL API. However, it is still interesting to perform this exercise on a few examples.

\subsection{Example}
We will start with the task of calculating the checksum of a buffer, as this is an example provided in the UPMEM documentation. We can therefore simply write the BPL version and compare the LOC in both cases.

Here is the UPMEM SDK version. First the DPU part (with all comments removed):
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <mram.h>
#include <stdbool.h>
#include <stdint.h>
#define CACHE_SIZE 256
#define BUFFER_SIZE (1 << 16)
__mram_noinit uint8_t buffer[BUFFER_SIZE];
__host uint32_t checksum;
int main() {
  __dma_aligned uint8_t local_cache[CACHE_SIZE];
  checksum = 0;
  for (unsigned int bytes_read = 0; bytes_read < BUFFER_SIZE;) {
    mram_read(&buffer[bytes_read], local_cache, CACHE_SIZE);
    for (unsigned int byte_index = 0; (byte_index < CACHE_SIZE) 
		&& (bytes_read < BUFFER_SIZE); byte_index++, bytes_read++) {
      checksum += (uint32_t)local_cache[byte_index];
    }
  }
  return checksum;
}
\end{minted}
and the host part
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <dpu.h>
#include <assert.h>
#include <stdint.h>
#include <stdio.h>
#ifndef DPU_BINARY
#define DPU_BINARY "trivial_checksum_example"
#endif
#define BUFFER_SIZE (1 << 16)
void populate_mram(struct dpu_set_t set, uint32_t nr_dpus) {
  struct dpu_set_t dpu;
  uint32_t each_dpu;
  uint8_t *buffer = malloc(BUFFER_SIZE * nr_dpus);
  DPU_FOREACH(set, dpu, each_dpu) {
    for (int byte_index = 0; byte_index < BUFFER_SIZE; byte_index++) {
      buffer[each_dpu * BUFFER_SIZE + byte_index] = (uint8_t)byte_index;
    }
    buffer[each_dpu * BUFFER_SIZE] += each_dpu; 
    DPU_ASSERT(dpu_prepare_xfer(dpu, &buffer[each_dpu * BUFFER_SIZE]));
  }
  DPU_ASSERT(dpu_push_xfer(set, DPU_XFER_TO_DPU, "buffer", 0, BUFFER_SIZE, 
  	DPU_XFER_DEFAULT));
  free(buffer);
}
void print_checksums(struct dpu_set_t set, uint32_t nr_dpus) {
  struct dpu_set_t dpu;
  uint32_t each_dpu;
  uint32_t checksums[nr_dpus];
  DPU_FOREACH(set, dpu, each_dpu) {
    DPU_ASSERT(dpu_prepare_xfer(dpu, &checksums[each_dpu]));
  }
  DPU_ASSERT(dpu_push_xfer(set, DPU_XFER_FROM_DPU, "checksum", 0, 
  	sizeof(uint32_t), DPU_XFER_DEFAULT));
  DPU_FOREACH(set, dpu, each_dpu) {
    printf("[%u] checksum = 0x%08x\n", each_dpu, checksums[each_dpu]);
  }
}
int main() {
  struct dpu_set_t set, dpu;
  DPU_ASSERT(dpu_alloc(DPU_ALLOCATE_ALL, NULL, &set));
  DPU_ASSERT(dpu_load(set, DPU_BINARY, NULL));
  DPU_ASSERT(dpu_get_nr_dpus(set, &nr_dpus));
  populate_mram(set, nr_dpus);
  DPU_ASSERT(dpu_launch(set, DPU_SYNCHRONOUS));
  print_checksums(set, nr_dpus);
  DPU_ASSERT(dpu_free(set));
  return 0;
}
}\end{minted}
which totals $19+47$ lines, or $66$ in all.

Now, using the BPL API, we have:
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <bpl/core/Task.hpp>
template<class ARCH>
struct VectorChecksum : bpl::Task<ARCH>  {
    USING(ARCH);
    auto operator() (vector<uint32_t> const& v)  {
        return accumulate(v.begin(), v.end(), uint64_t{0});
    }
    static uint64_t reduce (uint64_t a, uint64_t b)  { return a+b; }
};
\end{minted}
and
\begin{minted}[bgcolor=bg,linenos]{c++}
#include <vector>
#include <cstdio>
#include <bpl/bpl.hpp>
int main() {
    std::vector<uint32_t> v;  
    for (size_t i=1; i<=1<<16; i++)  {  v.push_back(i);  }
    Launcher<ArchUpmem> launcher {1_dpu};
    printf ("checksum: %ld\n", launcher.run<VectorChecksum>(split(v)));
}
}\end{minted}
which totals $9+9$ lines, or $18$ in all. 

Thus, we have 66 LOC for the UPMEM SDK and 18 LOC for the BPL.

In addition to the significant reduction in code size with the BPL, code written using the BPL 
is also much more readable than with the UPMEM SDK. Indeed, reading the BPL code makes it immediately
clear that the task is to compute the checksum of a vector, which is far less obvious in the UPMEM SDK version, 
where all the low-level SDK calls reduce overall readability.

Of course, it would be worthwhile to extend this LOC comparison to other examples, but it seems fairly clear that the advantage would still favor the BPL.
The remaining point to determine is the overhead of the BPL compared to writing directly using the UPMEM SDK. 
If a BPL version were consistently 50\% slower than a SDK version, its usefulness could be limited.
In this regard, benchmark tests can provide a typology of algorithms that are likely to benefit from execution via the BPL on a UPMEM architecture, and conversely, those that might be penalized.

Related to readability, it is also conceivable that a developer could implement a simple algorithm using the BPL to run on a UPMEM architecture 
without needing to know the UPMEM SDK, which would represent a significant benefit of the BPL.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}