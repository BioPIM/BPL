{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873192e5-e73b-42a7-8f13-2b45f848dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display_markdown, Code, Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01c2e6-0c27-4fe2-993a-074e9a2381b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\hypersetup{linkcolor=black}\n",
    "\\tableofcontents\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907acf3-3f2a-423c-87fc-c6e3d25a35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSourceMetadata (filename):\n",
    "    result = \"\";\n",
    "    with open(filename) as f:\n",
    "        lines = [l.replace(\"\\n\",\"\") for l in f.readlines() if l.startswith(\"//\")];\n",
    "        state=\"none\"\n",
    "        [i0,i1]=[-1,-1]\n",
    "        ranges=[]\n",
    "        for i,l in enumerate(lines):\n",
    "            if l.startswith(\"// @\"):\n",
    "                state=\"running\";\n",
    "                if i0!=-1: ranges.append ([i0,i])  \n",
    "                i0=i;\n",
    "            elif state==\"running\":\n",
    "                if l == len(l)*\"/\":  # same character            \n",
    "                    ranges.append ([i0,i])\n",
    "                    [i0,i1]=[-1,-1]\n",
    "                    state=\"none\"\n",
    "        result={}\n",
    "        for i0,i1 in ranges:\n",
    "            sp = lines[i0][4:].split(\":\")\n",
    "            tag = sp[0]\n",
    "            val = \"\".join(sp[1:]) + \" \"\n",
    "            for i in range(i0+1,i1):\n",
    "                val += lines[i].replace(\"//\",\"\").strip() + \" \";\n",
    "            result[tag]=val.strip();\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8e8eb-da17-4312-ac04-e6108d9cb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSourceString (filename):\n",
    "    result = \"\";\n",
    "    with open(filename) as f:\n",
    "        lines = [l.replace(\"\\n\",\"\") for l in f.readlines() if not l.startswith(\"//\")];\n",
    "        running=0\n",
    "        [i0,i1,braces]=[0,0,0]\n",
    "        for i,l in enumerate(lines):\n",
    "            if running==0:\n",
    "                if \"template<class ARCH>\" in l:  [running,i0]=[1,i]\n",
    "            else:\n",
    "                braces += l.count(\"{\") - l.count(\"}\")\n",
    "                i1=i;\n",
    "                if braces==0 and running>1: break;\n",
    "                running += 1;\n",
    "        lines = [l for l in lines[i0:i1+1] if not l.strip().startswith(\"//\")] \n",
    "        result = \"\\n\".join(lines)\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b39cc1-0977-41aa-b347-4fff657f4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speedup(df):\n",
    "    dfTmp = pd.DataFrame()\n",
    "    for task in tasklist:\n",
    "        for arch in archs:\n",
    "            for i in df[\"input\"].unique():\n",
    "                tmp = df[[\"taskname\",\"arch\",\"unit/nbcomp\",\"time\",\"input\"]] [(df[\"taskname\"]==task) & (df[\"arch\"]==arch) & (df[\"input\"]==i)].copy()\n",
    "                if len(tmp)==0: continue;\n",
    "                tmp[\"speedup\"] = tmp[\"time\"].iloc[0] / tmp[\"time\"]\n",
    "                dfTmp = tmp if len(dfTmp)==0 else pd.concat ([dfTmp,tmp])\n",
    "    return df.merge (dfTmp, on=[\"taskname\", \"arch\", \"unit/nbcomp\",\"time\",\"input\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb920f3d-d7da-431a-8d8d-4450c69b11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, arch, changename=True, scaling=True):\n",
    "    if len(df)==0: return [df,\"\"];\n",
    "    unitCol = df[\"unit\"].iloc[0]\n",
    "    if arch==\"upmem\":\n",
    "        if scaling: df[\"unit/nbcomp\"] = df[\"unit/nbcomp\"]/64\n",
    "        df[\"unit\"] = \"rank\"\n",
    "        unitCol = df[\"unit\"].iloc[0]\n",
    "    if changename:\n",
    "        df = df.rename(columns={\"unit/nbcomp\":unitCol})\n",
    "    return [df, unitCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d5534-1591-41d6-987a-9d62f5e76463",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs   = [\"multicore\", \"upmem\"]\n",
    "tracesDir  = \"./traces\"\n",
    "taskSrcDir = \"../../unit/tasks\"\n",
    "\n",
    "files = glob.glob(\"traces/*.txt\");\n",
    "#files = [\"traces/SyracuseReduce.txt\"]\n",
    "#files = [\"traces/SyracuseVector2.txt\"]\n",
    "#files = [\"traces/SyracuseVector3.txt\"]\n",
    "#files = [\"traces/SyracuseReduce.txt\", \"traces/SyracuseVector.txt\"]\n",
    "files = [\"traces/VectorCreation.txt\"]\n",
    "files = [\"traces/VectorSerialize.txt\"]\n",
    "#files = [\"traces/{}.txt\".format(x) for x in config.keys()]\n",
    "files = [\"traces/VectorChecksum.txt\"]\n",
    "files = [\"traces/traces_a2ffb4a.txt\"]\n",
    "#files = [\"traces/foo.txt\"]\n",
    "\n",
    "stats = [];\n",
    "for f in files:\n",
    "    for idx,line in enumerate(open(f)):\n",
    "        line = \" \".join(line.split());\n",
    "        info = [x for x in line.strip().split(\" \")] \n",
    "        info = [info[i] for i in [1,3, 5,6,7, 9,11,12,13,14,16]]\n",
    "        stats.append (info)\n",
    "\n",
    "schema  = {\n",
    "    \"taskname\":\"str\", \n",
    "    \"arch\":\"str\", \n",
    "    \"unit\":\"str\", \n",
    "    \"unit/nbcomp\":\"int\", \n",
    "    \"unit/nbproc\":\"int\", \n",
    "    \"time\"        :\"float\", \n",
    "    \"time/launch\" :\"float\", \n",
    "    \"time/pre\"    :\"float\",  \n",
    "    \"time/post\"   :\"float\", \n",
    "    \"time/result\" :\"float\", \n",
    "    \"input\":\"object\"\n",
    "}\n",
    "df = pd.DataFrame(stats,columns=schema.keys()).astype(schema)\n",
    "\n",
    "tasklist = sorted(list(df[\"taskname\"].unique()))\n",
    "\n",
    "taskSrcFiles = [\"{}/{}.hpp\".format(taskSrcDir,t) for t in tasklist]\n",
    "taskSrcFiles = [f for f in taskSrcFiles if  \"description\" in  getSourceMetadata(f)]\n",
    "\n",
    "dfUpmem = df[(df[\"arch\"]==\"upmem\")].copy()\n",
    "dfMulti = df[(df[\"arch\"]==\"multicore\")].copy()\n",
    "\n",
    "[dfUpmem,xaxisUpmem] = normalize(dfUpmem,\"upmem\",     False)\n",
    "[dfMulti,xaxisMulti] = normalize(dfMulti,\"multicore\", False)\n",
    "\n",
    "xaxisMap = { \"multicore\":\"thread\", \"upmem\":\"rank\" }\n",
    "\n",
    "df = pd.concat([dfUpmem,dfMulti]);\n",
    "\n",
    "df = compute_speedup(df);\n",
    "\n",
    "timeDetails = [x for x in schema.keys() if x.startswith(\"time/\")]\n",
    "df[\"time/unknown\"] = df[\"time\"]\n",
    "for col in timeDetails:  df[\"time/unknown\"] = df[\"time/unknown\"] - df[col]\n",
    "\n",
    "config = {}\n",
    "for x in taskSrcFiles:\n",
    "    name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "    if name in tasklist:\n",
    "        config[name] = {\"source\":x}\n",
    "#taskSrcFiles\n",
    "#tasklist\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3db72-3cad-43d8-8fef-ac676652daf4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## BPL version\n",
    "The hash of the GIT repository used for this benchmark is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec35f19-f79d-4df4-b8fc-c6dadf69ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632f56f-b122-4e9d-8313-72237ebbf44c",
   "metadata": {},
   "source": [
    "# Benchmark suite\n",
    "We describe here the tests used for the benchmark. We try to use tests showing different parts such as:\n",
    "\n",
    "* pure calculus involving few memory management\n",
    "* intensive memory management, e.g. creation of vectors and dynamically adding items into them\n",
    "* intensive broadcast management in case of UPMEM arch, e.g. sending/returning a huge vector to/from the task\n",
    "\n",
    "For each test, we will study the following:\n",
    "\n",
    "* comparing Multicore vs Upmem in order to quickly compare the behaviour of the task for the two architectures\n",
    "* the scalability for each available architectures. For the different tasks, we display as a function of process units number the ratio between the execution time for 1 process unit divided by the execution time for N process units. In a perfect world, one should be close to linearity. Note the following: for `multicore` arch, the unit process are threads and for `UPMEM` arch, process units are tasklets; in case of `UPMEM` arch and in order to have more readable `x` axis, we will display the number of ranks instead of tasklets.\n",
    "* assessing execution times of the different parts during the running of a task with the `UPMEM` architecture.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af192c8-76c8-4792-b683-1012eba068be",
   "metadata": {},
   "source": [
    "The tests to be benchmarked are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500510de-4ef8-4c93-803a-ab3ec0641997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tasklist:\n",
    "    print (\"   {}\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246bef4-4e07-41cd-9c5a-f27949009440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelsize = 500;\n",
    "nbcols=3;\n",
    "\n",
    "def display_stacked(df,nbrows,nbcols):\n",
    "    df = df[df[\"arch\"]==\"upmem\"].copy()\n",
    "\n",
    "    [df,unitCol] = normalize(df,\"upmem\",scaling=False);\n",
    "    \n",
    "    df[\"time/unknown\"] = df[\"time\"]\n",
    "    for col in timeDetails:  df[\"time/unknown\"] -= df[col]\n",
    "    timeCategory = [*timeDetails,\"time/unknown\"]\n",
    "        \n",
    "    dfStacked = df [[\"rank\",\"input\",*timeCategory]].melt(id_vars=[\"rank\",\"input\"],var_name=\"time\",value_name=\"% time\")\n",
    "    fig = px.area(dfStacked, x=\"rank\", y=\"% time\", color=\"time\", groupnorm=\"percent\", facet_col=\"input\", facet_col_wrap=3)\n",
    "    fig.update_layout(width=pixelsize*nbcols,height=pixelsize*nbrows, hovermode=\"x unified\")\n",
    "    fig.update_traces(hovertemplate=\"%{y:4.1f}%\")\n",
    "    fig.update_layout(title=\"Benchmark for test '{}'\".format(task))\n",
    "    fig.show()\n",
    "\n",
    "def display_speeduparch(dftask):\n",
    "    tmp1=dftask[dftask[\"arch\"]==\"multicore\"]\n",
    "    tmp2=dftask[dftask[\"arch\"]==\"upmem\"]\n",
    "    tmp3=tmp1.merge(tmp2, on=[\"unit/nbcomp\", \"input\"])\n",
    "    tmp3[\"speedup\"] = tmp3[\"time_x\"] / tmp3[\"time_y\"]\n",
    "    fig = px.line(tmp3, x=\"unit/nbcomp\", y=\"speedup\", color=\"input\")\n",
    "    fig.update_layout(title=\"Speedup upmem/multicore\", yaxis_title=\"Speedup: T(multicore) / T(upmem)\")\n",
    "    fig.update_layout(title=\"Benchmark for test '{}'\".format(task))\n",
    "    fig.update_layout(hovermode=\"x unified\")\n",
    "    fig.update_layout(width=1000,height=800)\n",
    "    fig.update_traces(hovertemplate=\"%{y:4.1f}x\")\n",
    "    fig.update_xaxes(title_text=\"ranks/threads\")\n",
    "    fig.show()\n",
    "\n",
    "def generate4task (df, task):\n",
    "\n",
    "    #######################################################################################\n",
    "    section = \"## Test '{}'\".format(task)\n",
    "    display_markdown(section, raw=True)\n",
    "\n",
    "    metadata = getSourceMetadata(config[task][\"source\"]);\n",
    "\n",
    "    benchmark_multicore_split = int(metadata.get(\"benchmark-multicore-split\", \"1\"))\n",
    "    \n",
    "    #print (task, metadata, benchmark_multicore_split);\n",
    "    #return\n",
    "    \n",
    "    #######################################################################################\n",
    "    for key,val in metadata.items():\n",
    "        display_markdown(\"### {}\".format(key.capitalize()), raw=True)\n",
    "        print (val)\n",
    "\n",
    "    #######################################################################################\n",
    "    display_markdown(\"### Source code\", raw=True)\n",
    "    src = getSourceString(config[task][\"source\"]);\n",
    "    display(Code(src, language='cpp'))\n",
    "\n",
    "    #######################################################################################\n",
    "    display_markdown(\"### Multicore vs Upmem\", raw=True)\n",
    "    tmp = df[df[\"taskname\"]==task];\n",
    "    nbinputs = len(tmp[\"input\"].unique())\n",
    "    nbrows = round(nbinputs/nbcols)\n",
    "\n",
    "    fig = px.line(tmp, x=\"unit/nbcomp\", y=\"time\", color=\"arch\", facet_col=\"input\", facet_col_wrap=nbcols, facet_row_spacing=0.05, facet_col_spacing=0.05)\n",
    "    fig.update_yaxes(title_text=\"log(time)\", type=\"log\")\n",
    "    fig.update_xaxes(title_text=\"threads/ranks\")\n",
    "    fig.update_layout(title=\"Benchmark for test '{}'\".format(task))\n",
    "    fig.update_layout(width=pixelsize*nbcols,height=pixelsize*nbrows)\n",
    "    fig.update_layout(hovermode=\"x unified\")\n",
    "    fig.update_traces(hovertemplate=\"%{y:4.6f}\")\n",
    "    fig.show();\n",
    "\n",
    "    #######################################################################################\n",
    "    display_markdown(\"### Scalability per architecture\", raw=True)\n",
    "    dftask  = df[(df[\"taskname\"]==task)].copy()\n",
    "    if len(dftask)!=0:\n",
    "        fig = px.line(dftask, x=\"unit/nbcomp\", y=\"speedup\", color=\"input\", facet_col=\"arch\", facet_col_wrap=2)\n",
    "        fig.update_layout(width=2*600,height=600, title=\"Speedup\", yaxis_title=\"Speedup T(0)/T(x)\")\n",
    "        fig.update_layout(title=\"Benchmark for test '{}'\".format(task))\n",
    "        fig.update_layout(hovermode=\"x unified\")\n",
    "        fig.update_traces(hovertemplate=\"%{y:4.1f}x\")\n",
    "        fig.show()\n",
    "\n",
    "    #######################################################################################\n",
    "    display_markdown(\"### Speedup upmem vs multicore\", raw=True)\n",
    "    display_speeduparch (df[(df[\"taskname\"]==task)].copy())\n",
    "    \n",
    "    #######################################################################################\n",
    "    display_markdown(\"### BPL overheads\", raw=True)\n",
    "    dfUpmem = df[(df[\"arch\"]==\"upmem\") & (df[\"taskname\"]==task)].copy()    \n",
    "    nbinputs = len(dfUpmem[\"input\"].unique())\n",
    "    nbrows = round(nbinputs/nbcols)\n",
    "    display_stacked (dfUpmem, nbrows, nbcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b2f22-f435-40ac-9ef4-f9c1ec3de2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasklist:\n",
    "    generate4task (df, task);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46161190-c066-468c-ae4a-48f7b43aa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rawdata(df,task,arch):\n",
    "    tmp = df[(df[\"taskname\"]==task)&(df[\"arch\"]==arch)].copy();\n",
    "    tmp = tmp[[\"arch\",\"input\", \"unit\",\"unit/nbcomp\",\"time\",\"time/launch\", \"time/pre\",\"time/post\", \"time/result\",\"time/unknown\"]]\n",
    "    tmp = tmp.rename(columns={'unit/nbcomp': 'nbcomp'})\n",
    "    tmp[\"nbcomp\"] = tmp[\"nbcomp\"].astype(int)\n",
    "    for col in tmp.columns:\n",
    "        if col.startswith(\"time/\"):\n",
    "            tmp = tmp.rename(columns={col: col[5:]})\n",
    "    display(Markdown(tmp.to_markdown(index=False, floatfmt=\"9.5f\")))\n",
    "    \n",
    "display_markdown(\"## Raw data\", raw=True)\n",
    "for task in tasklist:\n",
    "    display_markdown(\"### Test {}\".format(task), raw=True)\n",
    "    display_rawdata (df,task,\"upmem\")\n",
    "    display_rawdata (df,task,\"multicore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7443d53-ad83-4d74-952d-b53d17613622",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert \"1.Benchmark.ipynb\"  \\\n",
    "--no-input  \\\n",
    "--to pdf    \\\n",
    "--LatexPreprocessor.title=\"Benchmark for the BPL library\" \\\n",
    "--LatexPreprocessor.author_names=\"BioPim\" \\\n",
    "--LatexPreprocessor.date=\"`date +\"%d/%m/%Y\"`\" \\\n",
    "--TagRemovePreprocessor.enabled=True  \\\n",
    "--TagRemovePreprocessor.remove_cell_tags remove_cell  \\\n",
    "2> /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70022d39-9423-415d-8a61-43753ecd5fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df369735-4d9a-4449-8557-32bb3078b3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
